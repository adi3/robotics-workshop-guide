[
{
	"uri": "http://example.org/setup/create_iam_users/",
	"title": "Create IAM Users",
	"tags": [],
	"description": "",
	"content": " From your AWS console, go to the IAM dashboard and select Users from the left column.  Click on Add User and provide a name for the new user. Allow both access types, leaving default selections for the password options. Then click on Next: Permissions.  Select Attach existing policies directly and choose the AdministratorAccess policy. Then click on Next: Tags.  Skip adding tags and click on Next: Review. Then click on Create User.  Copy all credentials from the next page and share these only with the teammate for whom you created this resource. The Secret Access Key and the Password will never be retrievable after this screen is closed. Email login instructions to your teammate for good measure.  Repeat these steps for each member of your team. The newly created user will appear in your list of IAM Users.\n Note that all IAM users use a dedicated sign-in URL to log in to the AWS account. This URL can always be retrieved from your IAM dashboard.\n"
},
{
	"uri": "http://example.org/overview/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": "Business outcome  Market products to customers based on their proximity to retail store locations.  Objectives  Learn to use the Boto3 SDK with the Amazon Location service Add store locations using a geocoded address and calculated buffer size (geofence) Add a geofence trigger to track presence Simulate customer movement and track location changes Send a coupon via Amazon Pinpoint SMS when a customer is nearby  Hints for the developer  Use the AWS CloudShell to perform CLI commands Look for the tag \u0026lt;COMPLETE THE CODE\u0026gt; where you need to update or solve for a command or piece of code. Build the solution in the us-east-1 (N. Virginia) AWS region  "
},
{
	"uri": "http://example.org/overview/location/",
	"title": "Amazon Location service",
	"tags": [],
	"description": "",
	"content": " Supports multiple devices per Tracker Tracker device position supports 2D coordinates in WGS84 coordinate system (same as GPS) Supports multiple geofences per Geofence Collection Supports Here and ESRI geocoding services Supports base map tiles for custom web UI  Track the state of devices with respect to a geofence "
},
{
	"uri": "http://example.org/overview/arch/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/setup/attach_access_policy/",
	"title": "Attach access policy",
	"tags": [],
	"description": "",
	"content": " Log in to your AWS account with the credentials provided to you in the previous step. Go to IAM Users, then click on your user name to open the resource summary page. Select Add inline policy on the right.  On the Create policy page, choose the JSON tab and replace its contents with the following snippet.  { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::517502204741:role/RekognitionForRoboticsWorkshop\u0026quot; } } Click on Review policy. For name, type in RekognitionExternalAccess. Then hit the Create policy button.   The newly added inline policy should now be listed under the Permissions policies section in the user summary page.\n"
},
{
	"uri": "http://example.org/setup/",
	"title": "Setup",
	"tags": [],
	"description": "",
	"content": "Setup  Create IAM users Attach access policy Launch Cloud9 IDE Set up ROS project Run blank application  "
},
{
	"uri": "http://example.org/build-simulation/",
	"title": "Build Simulation",
	"tags": [],
	"description": "",
	"content": "Build Simulation  Broadcast robot description Bring up simulation world Set up robot controllers Visualize robot data Integrate planning toolkit Test robot movement Start image capture service  "
},
{
	"uri": "http://example.org/add-intelligence/",
	"title": "Add Intelligence",
	"tags": [],
	"description": "",
	"content": "Add Intelligence  Confirm Rekognition model access Snap image from camera stream Locate and identify objects Obtain physical coordinates Command robot to fetch objects  "
},
{
	"uri": "http://example.org/setup/launch_cloud9/",
	"title": "Launch Cloud9 IDE",
	"tags": [],
	"description": "",
	"content": " Go to the AWS RoboMaker dashboard and select Development environments from the left column.  Click on Create development environment, provide a name for the resource, and choose Melodic for the ROS distribution. Select c5.9xlarge for instance type, leaving all other options as default. Then click Create.  The IDE will open up in a new tab and spin up any required resources. This usually takes a few minutes. Once the IDE is active, click on the gear icon in the top-right corner of the window.  In the Preferences tab, scroll down to select AWS Settings, then choose Credentials. Toggle the switch against AWS managed temporary credentials. We want this setting disabled for the exercise.  Add your IAM user credentials to the machine via the terminal.  aws configure Follow the prompts and enter your Access Key ID and Secret Access Key. When prompted for a default region name, type in us-east-1. For default output format, type in json.\nSet up a supplementary CLI profile that allows external Rekognition access.  aws configure set profile.rekognition_access.role_arn \\ arn:aws:iam::517502204741:role/RekognitionForRoboticsWorkshop aws configure set profile.rekognition_access.source_profile default aws configure set profile.rekognition_access.region eu-central-1  Before we begin developing, confirm the ROS version installed by issuing the following command in the terminal:\nrosversion -d The terminal should print out melodic.\n"
},
{
	"uri": "http://example.org/model-training/",
	"title": "Model Training",
	"tags": [],
	"description": "",
	"content": "Model Training  Upload dataset to S3 Draw bounding boxes Initiate model training Evaluate training results Invoke trained model  "
},
{
	"uri": "http://example.org/setup/set_up_project/",
	"title": "Set up ROS project",
	"tags": [],
	"description": "",
	"content": " Create a workspace for the ROS project in our Cloud9 IDE.  mkdir -p ~/environment/aws_ws/src/ cd aws_ws/src Download starter code for our ROS application from GitHub.  git clone https://github.com/adi3/robomaker_workshop Install package dependencies. This step will take several minutes to complete.  cd ~/environment/aws_ws rosdep install --from-paths src --ignore-src --rosdistro melodic -r -y Set up Interbotix robot arm modules. Type no when prompted to set up the perception pipeline in the terminal.  cd ~/environment curl 'https://raw.githubusercontent.com/Interbotix/interbotix_ros_manipulators/main/interbotix_ros_xsarms/install/amd64/xsarm_amd64_install.sh' \u0026gt; xsarm_amd64_install.sh chmod +x xsarm_amd64_install.sh ./xsarm_amd64_install.sh Add our workspace setup to the .bashrc file. Then execute its contents.  echo “~/environment/aws_ws/src/robomaker_workshop/devel/setup.bash” \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc  Our ROS app is now configured and ready to be launched. Take a few moments to explore the code we downloaded in Step 2. Expand the directories in the explorer on the left, and double-click on files to view their contents.\n"
},
{
	"uri": "http://example.org/add-ons/",
	"title": "Add-Ons",
	"tags": [],
	"description": "",
	"content": "Add-Ons  Receive customer position updates with MQTT via AWS IoT Core  "
},
{
	"uri": "http://example.org/setup/run_blank_app/",
	"title": "Run blank application",
	"tags": [],
	"description": "",
	"content": " Install catkin build tools.  sudo apt update sudo apt install python-catkin-tools -y Build the ROS project.  cd ~/environment/aws_ws catkin build source devel/setup.bash Click on Virtual Desktop at the top, then select Launch Virtual Desktop to open up the Ubuntu desktop GUI.  Launch the ROS application, and confirm that it runs without errors. No GUI is expected to appear yet.  roslaunch robomaker_workshop main.launch  Explore application topography by looking at output from the following commands:\nrostopic list rosnode list rosparam list "
},
{
	"uri": "http://example.org/build-simulation/broadcast_robot_desc/",
	"title": "Broadcast robot description",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 1 of main.launch.  \u0026lt;param name=\u0026quot;robot_description\u0026quot; command=\u0026quot;xacro $(find robomaker_workshop)/urdf/$(arg robot_model).urdf.xacro\u0026quot;\u0026gt; \u0026lt;/param\u0026gt; \u0026lt;node name=\u0026quot;robot_state_publisher\u0026quot; pkg=\u0026quot;robot_state_publisher\u0026quot; type=\u0026quot;robot_state_publisher\u0026quot;\u0026gt; \u0026lt;/node\u0026gt; Run the ROS application.  roslaunch robomaker_workshop main.launch Confirm robot information is being published.  rosparam get -p /robot_description You should see some XML output printed in the terminal that describes the form-factor and physics of the robot.\n"
},
{
	"uri": "http://example.org/add-intelligence/confirm_model_access/",
	"title": "Confirm model access",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 1 of main.py.  rospy.loginfo(\u0026quot;Checking state of Rekognition model...\u0026quot;) status = util.model_status(ARN_BASE + PROJECT_ID, model_name, MODEL_ACCESS_PROFILE) rospy.loginfo('Current model state: %s' % status) if status != 'RUNNING': rospy.logerr('Rekognition model needs to be in RUNNING state') return Run the ROS application.  roslaunch robomaker_workshop main.launch Open a new terminal tab and head to the /scripts directory  cd ~/environment/aws_ws/src/robomaker_workshop/scripts Run the main.py script in simulation mode.  python main.py --sim The terminal output should report the current model to be in the RUNNING state.\n[INFO] [1623011969.371301, 0.000000]: Checking state of Rekognition model... [INFO] [1623011969.626026, 1734.312000]: Current model state: RUNNING  Ask your presenter in case you are shown an AccessDenied error or if you find that the model is not running.\n"
},
{
	"uri": "http://example.org/add-ons/iot_core/",
	"title": "Processing with IoT Core",
	"tags": [],
	"description": "",
	"content": "In this module you will learn how AWS IoT Core can receive position updates for a customer device and process them with Amazon Location.   Open the Lambda console and edit the function called \u0026ldquo;lbs_simulate_customers\u0026rdquo;. Comment out the line near line 96 that called \u0026ldquo;publish_location\u0026rdquo;.\n# publish_location(device)\nand uncomment the line near line 99 that calls \u0026ldquo;publish_location_iot\u0026rdquo;\npublish_location_iot(device)\n  Deploy the function code changes to save them.\n  Using a second browser window, navigate to the AWS IoT console and open the Test page. Subscribe to all topics using the wildcard # character.\n  Run the \u0026lsquo;lbs_simulate_customers\u0026rsquo; Lambda function using the default test event and immediately switch to the IoT Core test page to check if MQTT messages are arriving.\n  Add an IoT Core Rule that forwards customer device position updates to the \u0026lsquo;lbs_update_tracker\u0026rsquo; Lambda function. Click on Rules on the left and then enter the name lbs_customer_device_update with a topic filter of customers/#\n  Add a Rule Action for a Lambda function and select the \u0026lsquo;lbs_update_tracker\u0026rsquo; function from the list.\n  Save the new rule by clicking Create Rule.\n  Re-test the simulation by running the default test event and observing updates in DynamoDB as before. Updates should be occurring just as they had before, but now via MQTT with IoT Core.\n  "
},
{
	"uri": "http://example.org/model-training/upload_dataset/",
	"title": "Upload dataset to S3",
	"tags": [],
	"description": "",
	"content": " Fetch images of the real-world setup which is already prepared for you.  aws s3 cp s3://adsnghw-robotics/px100-dataset.zip . Unzip the downloaded archive.  unzip px100-dataset.zip Create a bucket on S3. You need to choose a name for your bucket that is globally unique. Refer to S3 naming rules for more information.  aws s3 mb s3://\u0026lt;YOUR_BUCKET_NAME\u0026gt; Upload the dataset to your S3 bucket.  aws s3 cp px100-dataset/ s3://\u0026lt;YOUR_BUCKET_NAME\u0026gt; --recursive Go to the S3 dashboard and confirm that your dataset has been uploaded. It should contain 50 images.  SCREENSHOT HERE  Optionally, clean your workspace by deleting the downloaded assets.\nrm -rf px100-dataset* "
},
{
	"uri": "http://example.org/build-simulation/bring_up_sim_world/",
	"title": "Bring up simulation world",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 2 of main.launch.  \u0026lt;include file=\u0026quot;$(find gazebo_ros)/launch/empty_world.launch\u0026quot;\u0026gt; \u0026lt;arg name=\u0026quot;world_name\u0026quot; value=\u0026quot;$(arg world_name)\u0026quot;/\u0026gt; \u0026lt;/include\u0026gt; Run the ROS application.  roslaunch robomaker_workshop main.launch Go to the virtual desktop to view our Gazebo world. You should see a black table with a few colorful coins on it. Take a few moments to familiarize yourself with the Gazebo GUI and the various options it offers.  Add the following code snippet under, once again, STEP 2 of main.launch.  \u0026lt;node pkg=\u0026quot;gazebo_ros\u0026quot; name=\u0026quot;urdf_spawner\u0026quot; type=\u0026quot;spawn_model\u0026quot; respawn=\u0026quot;false\u0026quot; output=\u0026quot;screen\u0026quot; args=\u0026quot;-urdf -model $(arg robot_model) -param robot_description -x $(arg x_offset) -z $(arg table_height)\u0026quot;\u0026gt; \u0026lt;/node\u0026gt; Run the ROS application again.  roslaunch robomaker_workshop main.launch Go to the virtual desktop and confirm that the robot arm appears in the simulated world.  "
},
{
	"uri": "http://example.org/model-training/draw_bboxes/",
	"title": "Draw bounding boxes",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 2 of main.py.  rospy.logwarn('Press Enter to snap image from ROS topic') raw_input() image = util.snap_image() if image == None: rospy.logerr('Trouble snapping image from ROS topic') return rospy.loginfo('Snapped image from local camera stream: %s' % image) Run the main.py script in simulation mode. Press Enter as prompted by the script.  python main.py --sim The terminal output will confirm that an image has been snapped.\n[INFO] [1623013252.228023, 667.865000]: Snapped image from local camera stream: image_0000.png  A snap of the camera stream should have appeared in your /scripts directory. Double-click on the file to view the image.\n"
},
{
	"uri": "http://example.org/add-intelligence/snap_camera_image/",
	"title": "Snap image from camera stream",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 2 of main.py.  rospy.logwarn('Press Enter to snap image from ROS topic') raw_input() image = util.snap_image() if image == None: rospy.logerr('Trouble snapping image from ROS topic') return rospy.loginfo('Snapped image from local camera stream: %s' % image) Run the main.py script in simulation mode. Press Enter as prompted by the script.  python main.py --sim The terminal output will confirm that an image has been snapped.\n[INFO] [1623013252.228023, 667.865000]: Snapped image from local camera stream: image_0000.png  A snap of the camera stream should have appeared in your /scripts directory. Double-click on the file to view the image.\n"
},
{
	"uri": "http://example.org/model-training/initiate_training/",
	"title": "Initiate model training",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 3 of main.py.  rospy.logwarn(\u0026#39;Press Enter to discover labels with Rekognition\u0026#39;) raw_input() labels = util.find_coins(image, model_arn, CONFIDENCE_THRESHOLD, MODEL_ACCESS_PROFILE) rospy.loginfo(\u0026#39;Found %dlabels in image\u0026#39; % len(labels)) util.print_labels(labels) Run the main.py script in simulation mode. Press Enter as prompted by the script.  python main.py --sim The terminal will print out details of the objects detected by Amazon Rekognition.\nInstall the ImageMagick package so that our python script can visualize labels.  sudo apt install imagemagick Add the following line at the end of STEP 3 of main.py.  util.display_labels(image, labels) Run the main.py script. Press Enter as prompted by the script.  python main.py --sim Head over to the virtual desktop and you will see the snapped image superimposed with the labels detected by our machine learning model.\n Our model is thus able to reliably\n identify the right number of objects in the image categorize the objects as the correct type, and locate each object at its precise location  "
},
{
	"uri": "http://example.org/add-intelligence/locate_identify_objects/",
	"title": "Locate and identify objects",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 3 of main.py.  rospy.logwarn(\u0026#39;Press Enter to discover labels with Rekognition\u0026#39;) raw_input() labels = util.find_coins(image, model_arn, CONFIDENCE_THRESHOLD, MODEL_ACCESS_PROFILE) rospy.loginfo(\u0026#39;Found %dlabels in image\u0026#39; % len(labels)) util.print_labels(labels) Run the main.py script in simulation mode. Press Enter as prompted by the script.  python main.py --sim The terminal will print out details of the objects detected by Amazon Rekognition.\nInstall the ImageMagick package so that our python script can visualize labels.  sudo apt install imagemagick Add the following line at the end of STEP 3 of main.py.  util.display_labels(image, labels) Run the main.py script. Press Enter as prompted by the script.  python main.py --sim Head over to the virtual desktop and you will see the snapped image superimposed with the labels detected by our machine learning model.\n Our model is thus able to reliably\n identify the right number of objects in the image categorize the objects as the correct type, and locate each object at its precise location  "
},
{
	"uri": "http://example.org/cleanup/set_up_controllers/",
	"title": "Set up robot controllers",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 3 of main.launch.  \u0026lt;rosparam command=\u0026quot;load\u0026quot; file=\u0026quot;$(find robomaker_workshop)/config/controllers.yaml\u0026quot;\u0026gt; \u0026lt;/rosparam\u0026gt; \u0026lt;node pkg=\u0026quot;controller_manager\u0026quot; name=\u0026quot;controller\u0026quot; type=\u0026quot;controller_manager\u0026quot; respawn=\u0026quot;false\u0026quot; output=\u0026quot;screen\u0026quot; args=\u0026quot;spawn arm_controller gripper_controller joint_state_controller\u0026quot;\u0026gt; \u0026lt;/node\u0026gt; Run the ROS application. No GUI changes are expected at this point.  roslaunch robomaker_workshop main.launch Confirm that the interface for executing robot control has been successfully set up.  rostopic list | grep controller More than two dozen ROS topics spawned by the controller_manager package should appear in the output.\nOur Gazebo simulation will now show torques being applied to the virtual robot arm.  "
},
{
	"uri": "http://example.org/model-training/evaluate_results/",
	"title": "Evaluate training results",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 4 of main.py.  rospy.logwarn(\u0026#34;Press Enter to transform coin positions into physical coordinates\u0026#34;) raw_input() rospy.loginfo(\u0026#39;Transforming pixels to physical coordinates...\u0026#39; % len(labels)) coins = {} for l in labels: name = l[\u0026#39;Name\u0026#39;] x, y = util.get_coin_position(l[\u0026#39;Geometry\u0026#39;][\u0026#39;BoundingBox\u0026#39;]) rospy.loginfo(name) rospy.loginfo(\u0026#39;\\tX: \u0026#39; + str(x)) rospy.loginfo(\u0026#39;\\tY: \u0026#39; + str(y)) coins[name] = [x, y] Run the main.py script in simulation mode. Press Enter as prompted by the script.  python main.py --sim You will now see the physical XY-coordinates of the coin\u0026rsquo;s location relative to the center of the table. These coordinates are measured in meters.\n The robot can now be informed of locations on the table that it needs to move to for collecting the coins.\n"
},
{
	"uri": "http://example.org/add-intelligence/obtain_physical/",
	"title": "Obtain physical coordinates",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 4 of main.py.  rospy.logwarn(\u0026#34;Press Enter to transform coin positions into physical coordinates\u0026#34;) raw_input() rospy.loginfo(\u0026#39;Transforming pixels to physical coordinates...\u0026#39; % len(labels)) coins = {} for l in labels: name = l[\u0026#39;Name\u0026#39;] x, y = util.get_coin_position(l[\u0026#39;Geometry\u0026#39;][\u0026#39;BoundingBox\u0026#39;]) rospy.loginfo(name) rospy.loginfo(\u0026#39;\\tX: \u0026#39; + str(x)) rospy.loginfo(\u0026#39;\\tY: \u0026#39; + str(y)) coins[name] = [x, y] Run the main.py script in simulation mode. Press Enter as prompted by the script.  python main.py --sim You will now see the physical XY-coordinates of the coin\u0026rsquo;s location relative to the center of the table. These coordinates are measured in meters.\n The robot can now be informed of locations on the table that it needs to move to for collecting the coins.\n"
},
{
	"uri": "http://example.org/build-simulation/visualize_data/",
	"title": "Visualize robot data",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 4 of main.launch.  \u0026lt;node name=\u0026quot;rviz\u0026quot; pkg=\u0026quot;rviz\u0026quot; type=\u0026quot;rviz\u0026quot; respawn=\u0026quot;false\u0026quot; output=\u0026quot;screen\u0026quot; args=\u0026quot;-f world -d $(find robomaker_workshop)/rviz/px100.rviz\u0026quot;\u0026gt; \u0026lt;/node\u0026gt; Run the ROS application.  roslaunch robomaker_workshop main.launch The RViz GUI should appear on the virtual desktop. Take a few moments to explore its various components, and get an understanding of how RViz shows the camera livestream from our simulated Gazebo world.  "
},
{
	"uri": "http://example.org/cleanup/aws_resources/",
	"title": "AWS Resources Cleanup",
	"tags": [],
	"description": "",
	"content": "Now that you have finished with the workshop we want to make sure that you clean up all the resources that were deployed.\n Navigate the AWS CloudFormation console and Delete the stack created earlier.  "
},
{
	"uri": "http://example.org/add-intelligence/command_robot/",
	"title": "Command robot to fetch",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 5 of main.launch.  rospy.logwarn(\u0026quot;Press Enter to instruct robot to pick a coin\u0026quot;) raw_input() robot = PX100(simulated = _sim) for name, position in coins.items(): robot.home() robot.open_gripper() x = position[0] y = position[1] rospy.loginfo(\u0026quot;Picking up %s...\u0026quot; % name) success = robot.go_to([x, y, 0.01]) if success: robot.close_gripper() robot.home() robot.deposit() rospy.loginfo(\u0026quot;No more coins. Going to sleep...\u0026quot;) robot.sleep() Run the main.py script in simulation mode. Press Enter as prompted by the script.  python main.py --sim The simulated robot arm will calculate an appropriate trajectory for moving to a coin\u0026rsquo;s location, pick up the coin, and deposit it at a pre-defined location. The robot will do this for each of the coins found in the previous steps.\n In case the robot is unable to grasp the coin in its first attempt, simply rerun the script. Since our machine learning model is built to handle dynamic scenarios, the coin will be accurately located even if it has been randomly nudged around by the robot arm during previous attempts.\n"
},
{
	"uri": "http://example.org/build-simulation/integrate_planning/",
	"title": "Integrate planning toolkit",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 5 of main.launch.  \u0026lt;include file=\u0026quot;$(find interbotix_xsarm_moveit)/launch/move_group.launch\u0026quot;\u0026gt; \u0026lt;arg name=\u0026quot;robot_model\u0026quot; value=\u0026quot;$(arg robot_model)\u0026quot;/\u0026gt; \u0026lt;arg name=\u0026quot;dof\u0026quot; value=\u0026quot;$(arg dof)\u0026quot;/\u0026gt; \u0026lt;/include\u0026gt; Run the ROS application. No GUI changes are expected at this point.  roslaunch robomaker_workshop main.launch Confirm that the planning interface has been succesfully set up.  rostopic list | grep move_group About twenty new ROS topics should have been set up by MoveIt to allow for motion planning.\nInterestingly, MoveIt adds a large number of key-value pairs to the ROS Parameter Server. These are used by MoveIt algorithms for planning and executing trajectories with a robot.  rosparam list | grep move_group Well over a hundred ROS parameters should now be printed in the terminal. You do not need to understand these parameters for the purposes of this workshop.\n"
},
{
	"uri": "http://example.org/model-training/invoke_model/",
	"title": "Invoke trained model",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 5 of main.launch.  rospy.logwarn(\u0026quot;Press Enter to instruct robot to pick a coin\u0026quot;) raw_input() robot = PX100(simulated = _sim) for name, position in coins.items(): robot.home() robot.open_gripper() x = position[0] y = position[1] rospy.loginfo(\u0026quot;Picking up %s...\u0026quot; % name) success = robot.go_to([x, y, 0.01]) if success: robot.close_gripper() robot.home() robot.deposit() rospy.loginfo(\u0026quot;No more coins. Going to sleep...\u0026quot;) robot.sleep() Run the main.py script in simulation mode. Press Enter as prompted by the script.  python main.py --sim The simulated robot arm will calculate an appropriate trajectory for moving to a coin\u0026rsquo;s location, pick up the coin, and deposit it at a pre-defined location. The robot will do this for each of the coins found in the previous steps.\n In case the robot is unable to grasp the coin in its first attempt, simply rerun the script. Since our machine learning model is built to handle dynamic scenarios, the coin will be accurately located even if it has been randomly nudged around by the robot arm during previous attempts.\n"
},
{
	"uri": "http://example.org/cleanup/",
	"title": "Wrap Up",
	"tags": [],
	"description": "",
	"content": "Wrap Up  Delete AWS Resources via CloudFormation Manually delete AWS resources created by the code  "
},
{
	"uri": "http://example.org/build-simulation/test_movement/",
	"title": "Test robot movement",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 6 of main.launch.  \u0026lt;node pkg=\u0026quot;robomaker_workshop\u0026quot; name=\u0026quot;init\u0026quot; type=\u0026quot;init.py\u0026quot; output=\u0026quot;screen\u0026quot; /\u0026gt; Run the ROS application.  roslaunch robomaker_workshop main.launch Check out the Gazebo window. The simulated arm should become animated and move to its sleep position.  The same effect can be achieved by running the python script independently of the launch file.\nComment out the code added under Step 6 of main.launch.  \u0026lt;!-- \u0026lt;node pkg=\u0026quot;robomaker_workshop\u0026quot; name=\u0026quot;init\u0026quot; type=\u0026quot;init.py\u0026quot; output=\u0026quot;screen\u0026quot; /\u0026gt; --\u0026gt; Run the application again. The robot arm will appear in its default upright position.  roslaunch robomaker_workshop main.launch Now execute the init.py script from the /scripts folder.  cd ~/environment/aws_ws/src/robomaker_workshop/scripts python init.py Head over to the virtual desktop and you will see the arm once again curl up as previously to its sleep position.\n Before proceeding, remember to add back the init node statement in the launch file. We will start all our robot operations from this sleep position from now on.\n"
},
{
	"uri": "http://example.org/build-simulation/start_image_capture/",
	"title": "Start image capture service",
	"tags": [],
	"description": "",
	"content": " Add the following code snippet under STEP 7 of main.launch.  \u0026lt;node name=\u0026quot;image_saver\u0026quot; pkg=\u0026quot;image_view\u0026quot; type=\u0026quot;image_saver\u0026quot; output=\u0026quot;screen\u0026quot;\u0026gt; \u0026lt;remap from=\u0026quot;image\u0026quot; to=\u0026quot;camera/image_raw\u0026quot; /\u0026gt; \u0026lt;param name=\u0026quot;save_all_image\u0026quot; value=\u0026quot;false\u0026quot; /\u0026gt; \u0026lt;param name=\u0026quot;filename_format\u0026quot; value=\u0026quot;$(find robomaker_workshop)/scripts/image_%04d.png\u0026quot; /\u0026gt; \u0026lt;/node\u0026gt; Run the ROS application.  roslaunch robomaker_workshop main.launch Confirm that the image capture service has started.  rosservice list | grep image_saver Of particular interest to us is the /image_saver/save service. We will be invoking this service to captures images from the live camera stream.\n"
},
{
	"uri": "http://example.org/cleanup/signup/",
	"title": "Sign up",
	"tags": [],
	"description": "",
	"content": "Periodically we may have updates to this workshop that reflect improvements to existing modules. This might occur due to a new service or feature that simplifies how you accomplish an outcome we\u0026rsquo;ve addressed. We also plan to add additional modules that extend the base workshop.\nSend an email to kevinol@amazon.com if you\u0026rsquo;re interested in receiving an email update when changes are released.\n"
},
{
	"uri": "http://example.org/",
	"title": "Title",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]